
# 職務経歴書

-   [略歴](https://github.com/five-dots/curriculum-vitae#略歴)
-   [概要](https://github.com/five-dots/curriculum-vitae#概要)
-   [技術](https://github.com/five-dots/curriculum-vitae#スキル)
-   [主な業務経験](https://github.com/five-dots/curriculum-vitae#主な業務経験)

## 略歴

-   **five-dots**: 
    -   フリーランス・データエンジニア・アナリティクスエンジニア・データサイエンティスト ([GitHub リポジトリ](https://github.com/five-dots?tab=repositories) | [Blog](https://objective-boyd-9b8f29.netlify.app/) | [Qiita](https://qiita.com/five-dots))

-   **2007 年**: 早稲田大学 政治経済学部 政治学科 卒業。
    -   研究テーマは、日本の選挙制度の計量分析。(SPSS を利用した計量分析)
    -   コンピューター系のサークルに所属し、Linux サーバー構築・ PC 自作などを楽しむ。

-   **2007 年**: シスコシステムズ株式会社に入社。
    -   約 10 年間、国内の大手通信キャリアを担当し、大規模コアネットワークの導入を支援。
    -   結果へのコミットメント、多くの利害関係者と協業、最先端の技術を実ビジネスに展開する困難さなど、たくさんの貴重な経験を会得。

-   **2016 年**: フリーランスエンジニア
    -   自身が代表を務める法人を設立。
    -   米国金融マーケットへのアルゴリズム投資戦略開発を主としたフリーランスエンジニアとして新たなキャリアをスタート (2016 年〜)
    -   データサイエンティスト兼データエンジニアとして、モデル開発やデータ基盤の整備、発注システムの開発などを行う。
    -   金融領域以外にも、位置情報を利用した小売業界向け予測アルゴリズムの開発などに従事 (2020年〜)

## 概要

-   [ **フルスタックのデータサイエンス** ] データの収集・取得から、前処理、特徴量設計、モデル構築、クラウド環境利用、パフォーマンスチューニングと評価、レポーティングやダッシュボード開発までのデータ分析プロジェクトのフローを完結可能です。

-   [ **モダンデータスタックの活用** ] クラウドネイティブなデータ管理ツールを利用し、運用コストを最小限に抑えたデータ分析基盤の構築のご提案が可能です。

-   [ **分析のコード化** ] アドホックなデータ分析などの「アナリスト」的な業務だけでなく、分析タスクをコード化していく「エンジニア」寄りのタスクに強みがあります。

-   [ **コミュニケーション力・チームプレイ** ] コーディングやデータ分析業務だけでなく、社内外の多様な部門との協業による案件推進や、プロジェクトマネジメント、顧客へのプレゼンテーションや折衝まで対応可能です。

## スキル

利用頻度が高く、特に得意としているものは、**太字**で記載。

### Language

-   **Python**, **R**, **SQL**, **Shell Script**, Go, C#, Stan

### DWH/RDBMS

-   **BigQuery**, **Snowflake**, RedShift

-   PostgreSQL, MySQL, SQL Server, SQLite

### Cloud Platform

-   Google Cloud
    -   **BigQuery**, **Cloud Storage**, **Compute Engine**, **Dataproc**, **Cloud Functions**, **Cloud Run**, **Pub/Sub**, **GKE**, Batch, Cloud SQL, Container Registry, Artifact Registry, Cloud Scheduler, VPC, IAM, Secret Manager, Logging, Monitoring, Google Maps Platform

-   AWS
    -   EC2, S3, RDS, Lambda, Redshift

### Data Stack

-   **dbt**, dbt Cloud, Dataform, Fivetran, Airbyte
-   **Spark (PySpark, SparkR)**
-   **Airflow**, **Argo Workflows**, Prefect, Rundeck
-   OpenMetaData

### BI/Dashboard/Notebook/Visualization

-   Tableau, Google Data Studio (Looker Studio), Superset, Metabase, Mode
-   **Streamlit**, **Shiny**
-   **Jupyter Lab**, Jupyter Notebook, Hex, Rmarkdown

### Version Control and CI/CD

- GitHub, GitLab, Bitbucket
- GitHub Actions, Bitbucket Pipelines, Travis CI

### その他

-   Docker, Docker Compose, Dev Container
-   QGIS

## 主な業務経験

### 小売業界向けダッシュボードのバッチ再構築プロジェクト (SQL/R)

【業務内容と技術要素】

-   小売業界向け SaaS プロダクトのデータマート生成バッチ再構築プロジェクト
-   位置情報を利用した、小売店舗の来店人数や来店顧客の特性分析の推定モデルを改善
-   既存の BigQuery + gcloud cli (Shell Script) の構成から、dbt を使ったパイプライン構築へ変更
-   技術要素として、dbt, dbt Cloud, BigQuery, BQML, R など 
-   データマートのバージョン管理・データテストの実装によりデータ品質とデプロイまでのスピードが大きく向上

### 位置情報を利用したユーザーのプロファイル推定アルゴリズムの開発 (SQL/R/Python)

【業務内容と技術要素】

-   スマートフォンからの位置情報を利用してユーザーの居住地・勤務地・性別・年代・行動趣向性を推定するアルゴリズムを開発
-   データの収集・前処理・特徴量設計・モデル構築・パフォーマンスチューニング・評価・経営層へのレポーティングまで一気通貫に担当
-   技術要素としては、BigQuery, BQML, Compute Engine, R, Airflow などを活用
-   ユーザーデモグラの推定モデルとして GBDT 系モデル (BQML) を採用し、従来モデルから大きく精度を改善

### 分析用データ ETL タスクのサーバーレス化 (Python)

【業務内容と技術要素】

-   分析用データ変換タスクをコスト効率よく実行するため、主に GCP Cloud Functions と Cloud Run を活用したサーバーレス構成で実装
-   タスクスケジューラーとしては、 Airflow を採用
-   従来の Compute Engine を利用したデータバッチの運用に比べて、運用コストが大きく低下

### 位置情報を利用した食品メーカー向けダッシュボード構築 (SQL/Python)

【業務内容と技術要素】

-   位置情報を利用し、小売店の来店人数の推定や来店顧客のプロファイリングを行うメーカー向けダッシュボード構築
-   プロジェクトマネージャー兼、データエンジニアとして参画
-   既存の BI ツールである QlikView から Tableau への移行を支援
-   Tableau 向けのデータマートを BigQuery 上に構築する作業を担当
-   データに対する検証 (テスト) を実装することで、2ヶ月という短期間で品質を確保しながら移行を完了

### 機械学習プロジェクトのためのメタパッケージ開発 (R)

【業務内容と技術要素】

-   データ分析における「属人的なノウハウ」を誰にでも利用可能・再現可能にするためのパッケージを開発
-   R 言語を利用し、パッケージを開発 (現在はアルファ版のステージ)
-   機械学習プロジェクトの複雑なパラメタを管理し、クロスバリデーション〜アンサンブルまでを行う
-   実際には、以下の項目をパラメタとして管理し、ベストモデルの選定からスタッキングまでをサポート
    -   [ **データセット** ] 同一タスクでも特徴量の異なるデータセットを複数入力可能
    -   [ **モデル** ] 主要な教師あり機械学習モデルパッケージへの対応
    -   [ **パイパーパラメタ** ] モデル毎に取り得るパラメタの範囲のベストプラクティスの事前設定と実際の探索アルゴリズムパッケージを内部で利用可能 (グリッドサーチ・ランダムサーチ・ベイズ最適化)
    -   [ **前処理手法** ] モデル毎に必要な前処理手法のベストプラクティスの調査と実装
    -   [ **リサンプル手法** ] クロスバリデーションのための複数のリサンプル手法に対応
    -   [ **乱数シード** ] 同一モデルで乱数だけを変更したアンサンブルに対応
-   プロジェクト毎の重複作業を削減し、かつ検討漏れもなくしていくことが可能

### 機械学習分析結果のダッシュボード開発 (R/Python)

【業務内容と技術要素】

-   R + Shiny を利用したダッシュボード (フロント) 開発
-   Zappa (Python + Flask + AWS API Gateway + Lambda + RDS) を利用したサーバーレスでの機械学習 Web API の整備
-   日次の分析タスクを GCP の Compute Engine を利用してオフロード

### 投資アルゴリズムの調査・研究・開発 (R/Python)

【業務内容と技術要素】

-   R, Ptyhon を利用した投資アルゴリズムの調査研究業務
-   海外ジャーナル・書籍・ Web ・ Kaggle などの調査研究
-   主に R を使った、探索的なデータ解析 (EDA) とバックテストの経験
-   時系列クロスバリデーション手法等を利用した、過学習に陥らない評価手法

### 米国 ETF (上場投資信託) のポートフォリオ構築プラットフォーム (R/Stan/C#)

【業務内容と技術要素】

-   R + Stan で時系列モデルを構築し、収益率とボラティリティ (変動幅) の予測モデルを構築
-   古典的な ARMA モデルから、階層ベイズモデル、マルコフ転換モデルなどの様々なモデルを実装
-   ボラティリティ予測は、GARCH, GJR, eGARCH など複数のモデルを実装
-   予測された収益率とボラティリティをベースに CCC, DCC, Copula などの手法を利用して最適な資産配分を決定
-   C# で 証券会社の API に接続し、推定された資産配分になるようオーダー処理を実装し、自動でリバランスを実施
-   日々のパフォーマンスをトラッキングし、分析レポートを RMarkdown で自動生成

### 米国マーケットデータ基盤の構築 (C#)

【業務内容と技術要素】

-   Entity Framework + Miscrosoft SQL Server を利用した自社のデータ基盤構築 (数千万件レベル)
-   将来のデータ種別の追加や要件に変更に備えたテーブル設計
-   パフォーマンス向上のための、カラムストアインデックスの調査 (実際には導入せず)
-   外部のデータベンダー の API との接続モジュールを C# で開発
-   C# + HtmlAgilityPack を利用した
-   バッチスクリプトの開発とジョブ管理
